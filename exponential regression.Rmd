---
title: "Statistical Computing - Exponential regression"
author: "Eric Roseren"
date: "4/23/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Leukemia data set

The data below from Feigl and Zelen (1965) are time to death, Y (weeks), in weeks from
diagnois and log10(wbc) (the initial white blood cell count), x (lwbc), for 17 patients
suffering from leukemia. The relation between Y and x is the main aspect of interest.

```{r}
# load the data
eleuk <- read.table("http://statacumen.com/teach/SC1/SC1_HW11_Exp-leuk.dat", header = TRUE)
```

```{r}
f.exp.l <- function(y, X, beta) {
# Exponential regression, log-likelihood
# standard linear model objects: y, X, beta
# center the Xs
#X[,2] <- X[,2] - mean(X[,2])
#X <- X - matrix(rep(1, nrow(X)), ncol = 1) %*% colMeans(X)
# exponential log-likelihood
l <- sum(-(X %*% beta + y * exp(-X %*% beta)))
return(l)
}
```


```{r}
f.exp.NR <- function(X, y, beta.1, method = "F", eps1 = 1e-6, eps2 = 1e-7, maxit = 100) {
# NR/Fisher's Scoring routine for estimation of Exponential model (with line search)
# Input:
# X = n-by-(r+1) design matrix
# y = n-by-1 vector of success counts
# beta.1 = (r+1)-by-1 vector of starting values for regression est
# method = "N" Newton-Raphson or "F" Fisher's Scoring
# Iteration controlled by:
# eps1 = absolute convergence criterion for beta
# eps2 = absolute convergence criterion for log-likelihood
# maxit = maximum allowable number of iterations
# Output:
# out = list containing:
# beta.MLE = beta MLE
# NR.hist = iteration history of convergence differences
# beta.hist = iteration history of beta
# beta.cov = beta covariance matrix (inverse Fisher's information matrix at MLE)
# note = convergence note
beta.2 <- rep(-Inf, length(beta.1)) # init beta.2
diff.beta <- sqrt(sum((beta.1 - beta.2)^2)) # Euclidean distance
llike.1 <- f.exp.l(y, X, beta.1) # update loglikelihood
llike.2 <- f.exp.l(y, X, beta.2) # update loglikelihood
diff.like <- abs(llike.1 - llike.2) # diff

if (is.nan(diff.like)) { diff.like <- 1e9 }

i <- 1 # initial iteration index

alpha.step <- seq(-1, 2, by = 0.1)[-11] # line search step sizes, excluding 0
NR.hist <- data.frame(i, diff.beta, diff.like, llike.1, step.size = 1) # iteration history
beta.hist <- matrix(beta.1, nrow = 1)

while ((i <= maxit) & (diff.beta > eps1) & (diff.like > eps2)) {
        i <- i + 1 # increment iteration
        # update beta
        beta.2 <- beta.1 # old guess is current guess
        mu.2 <- exp(X %*% beta.2) # mean
        score.2 <- t(X) %*% diag(as.vector(1/mu.2)) %*% (y - mu.2) # score function, first derivative
        ldd <- -t(X) %*% diag(as.vector(y / mu.2)) %*% X # second derivative
        EI <- t(X) %*% X # expected information
        # this increment version solves for (beta.2-beta.1) without inverting Information
        if (method == "N") { # NR
                increm <- - solve(ldd, score.2) # solve for increment
        }
        if (method == "F") { # Fisher's Scoring
                increm <- solve( EI, score.2) # solve for increment
        }
        # line search for improved step size
        llike.alpha.step <- rep(NA, length(alpha.step)) # init llike for line search
        for (i.alpha.step in 1:length(alpha.step)) {
                llike.alpha.step[i.alpha.step] <- f.exp.l(y, X,beta.2 + alpha.step[i.alpha.step] * increm)
        }
        # step size index for max increase in log-likelihood (if tie, [1] takes first)
        ind.max.alpha.step <- which(llike.alpha.step == max(llike.alpha.step))[1]
        beta.1 <- beta.2 + alpha.step[ind.max.alpha.step] * increm # update beta
        diff.beta <- sqrt(sum((beta.1 - beta.2)^2)) # Euclidean distance
        llike.2 <- llike.1 # age likelihood value
        llike.1 <- f.exp.l(y, X, beta.1) # update loglikelihood
        diff.like <- abs(llike.1 - llike.2) # diff
        # iteration history
        NR.hist <- rbind(NR.hist, c(i, diff.beta, diff.like, llike.1, alpha.step[ind.max.alpha.step]))
        beta.hist <- rbind(beta.hist, matrix(beta.1, nrow = 1))
        }
# prepare output
out <- list()
out$beta.MLE <- beta.1
out$iter <- i - 1
out$NR.hist <- NR.hist
out$beta.hist <- beta.hist
out$beta.cov <- solve(EI) # variance matrix for betas
if (!(diff.beta > eps1) & !(diff.like > eps2)) {
        out$note <- paste("Absolute convergence of", eps1, "for betas and",eps2, "for log-likelihood satisfied")
}
if (i > maxit) {
        out$note <- paste("Exceeded max iterations of ", maxit)
}
return(out)
}
```



## Predictions

```{r}
eleuk <- read.table("http://statacumen.com/teach/SC1/SC1_HW11_Exp-leuk.dat", header = TRUE)
# create data variables: y, X
n <- nrow(eleuk)
y <- matrix(eleuk$weeks, ncol = 1)
X.temp <- eleuk$lwbc
# design matrix
X <- matrix(c(rep(1,n), X.temp), nrow = n)
colnames(X) <- c("Int", "lwbc")
r <- ncol(X) - 1 # number of regression coefficients - 1
# initial beta vector
beta.init <- c(0, rep(0, r))
# fit betas using our exponential regression NR/FS function
out.NR <- f.exp.NR(X, y, beta.init, "N")
out.NR
```


```{r}
f.exp.pred <- function(x, beta) {
# Exponential regression, predicted values
# univariate x, vector beta
# exponential log-likelihood
y <- beta[1] * exp(beta[2] * (x[,2] - colMeans(x)[2]))
#y <- exp(beta[1] + beta[2] * x)
return(y)
}
```


```{r}
# Plot at top of HW
library(ggplot2)
data <- data.frame(x=X)
pred.values <- data.frame(x=eleuk$lwbc,y=eleuk$weeks,y.pred=f.exp.pred(X,out.FS$beta.MLE))

p <- ggplot(eleuk, aes(x = lwbc, y = weeks)) +
  scale_y_continuous(breaks = 52*c(0,1,2,3)) +
 # geom_line(data = pred.values,inherit.aes = FALSE)+
  stat_function(fun = f.exp.pred, args = list(out.FS$beta.MLE),inherit.aes = F,color = 'red') +
  geom_point(alpha = 0.5, size = 3) +
  labs(title = "Leukemia: weeks survival by log white blood cell count")

print(p)
```

