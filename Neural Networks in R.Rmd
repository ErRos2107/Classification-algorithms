---
title: "Classification algorithm - Neural Networks"
author: "Eric Roseren"
date: "5/9/2019"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this section we will implement an L-layer neural network from scratch and use it for classification tasks. 

We will first start with a 1 layer Neural Network with a sigmoid activation function so that it is equivalent to a standart logistic regression function. 

We will then procede to increase the number of layer and unit cell to obtain a L- layer Neural Network with appropriate activation function and regularization method (L-2 norm, dropout) to reduce overfitting.

To load a dataset that is stored on an H5 file we can use the rhdf5 package from Bioconductor as follow:
```{r}
#source("http://bioconductor.org/biocLite.R") # Install package from bioconductor source
#biocLite("rhdf5")
library(rhdf5) # load package 
library(countcolors)  # plot (rgb array)
```

Guidelines:
*List the objects within the file to find the data group you want to read:
```{r} 
#h5ls("train_catvnoncat.h5")
f <- "train_catvnoncat.h5"
# view the structure of the H5 file
h5ls(f, all = TRUE)
```

*Read the HDF5 data:
```{r}
load.dataset <- function(){
#Standardised training set
train.x <- aperm(h5read("train_catvnoncat.h5","train_set_x"))/255
train.y <- t(h5read("train_catvnoncat.h5","train_set_y"))


# Standardised test set
test.x <- aperm(h5read("test_catvnoncat.h5","test_set_x"))/255
test.y <- t(h5read("test_catvnoncat.h5","test_set_y"))

# classes 

classes = h5read("test_catvnoncat.h5","list_classes")
output <- list(train.x=train.x,train.y=train.y,
               test.x=test.x,test.y=test.y,classes=classes)
return(output)
}

```
```{r}
original.data <- load.dataset()
```


## The data 

To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.

One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).

Let's visualise the 25th image:
```{r}
index <- 25
m <- original.data$train.x[index,,,]
result <-as.numeric( t(original.data$train.y[index]))
cat.noncat <- original.data$classes[result+1]
plotArrayAsImage(m, main = paste("y= ",result,", it is a ",cat.noncat," image ",sep = "" ))
```

We will be performing a multitude of matrix multiplication so having in mind the dimensions of the data training and test set will be helpful.
```{r}
m.train <- dim(original.data$train.x)[1]
m.test <- dim(original.data$test.x)[1]
num.px <- dim(original.data$train.x)[2]
```

Remember that train.set.x is an array of shape (m.train, num.px, num.px, 3).
To feed the images to the neural network, the dimension of the array needs to be flatten. The dimension of the new array will have the following shape:
(num.px $*$ num.px $*$  3, 1).
```{r}

train.x.flatten <- matrix(data = aperm(original.data$train.x),num.px*num.px*3,m.train)
test.x.flatten <- matrix(data = aperm(original.data$test.x),num.px*num.px*3,m.test)
```

## General Architecture of the learning algorithm

The following Figure explains why Logistic Regression is actually a very simple Neural Network!
![Logistic Regression Architecture](/users/eric/Documents/R programming/Classification-algorithms/figure/LogReg_kiank.png)
**Mathematical expression of the algorithm**:

For one example $x^{(i)}$:
$$z^{(i)} = w^T x^{(i)} + b \tag{1}$$
$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$ 
$$ \mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} )  \log(1-a^{(i)})\tag{3}$$

The cost is then computed by summing over all training examples:
$$ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}$$

## 4 - Building the parts of our algorithm ## 

The main steps for building a Neural Network are:
1. Define the model structure (such as number of input features) 
2. Initialize the model's parameters
3. Loop:
    - Calculate current loss (forward propagation)
    - Calculate current gradient (backward propagation)
    - Update parameters (gradient descent)
    
```{r}
# Sigmoid function

sigmoid <- function(z){
  "
    Compute the sigmoid of z

    Arguments:
    z -- A scalar or array of any size.

    Return:
    s -- sigmoid(z)
    "
  s <- 1/(1+exp(-z))
  return(s)
}
```
```{r}
print (paste("sigmoid([0, 2]) = ", as.numeric(sigmoid(array(data=c(0,2)))),sep = ""))
```
### 4.2 - Initializing parameters
We need to initialise the parameters w and b to zero:

```{r}
initialise.to.zero <- function(shape){
  "
    This function creates a vector of zeros of dimension (shape, 1) for w and initializes b to 0.
    
    Argument:
    dim -- size of the w vector we want (or number of parameters in this case)
    
    Returns:
    w -- initialized vector of shape (dim, 1)
    b -- initialized scalar (corresponds to the bias)
    "
 w <- matrix(data = rep(0,shape),nrow = shape,ncol = 1) 
 b <- 0
 result <- list(w=w,b=b)
 return(result)
}
```

### 4.3 - Forward and Backward propagation



Forward Propagation:
We get X:
 $$A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$$
 We calculate the cost function: $$J = \frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$$

Here are the two formulas : 

$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$
$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$
```{r}
propagate <- function(w, b, X, Y){
    "
    Implement the cost function and its gradient for the propagation explained above

    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)
    Y -- true label vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)

    Return:
    cost -- negative log-likelihood cost for logistic regression
    dw -- gradient of the loss with respect to w, thus same shape as w
    db -- gradient of the loss with respect to b, thus same shape as b
    

    "
    
    m <-  dim(X)[2]
    
    # FORWARD PROPAGATION (FROM X TO COST)
    A <- sigmoid((t(w) %*% X)+b)                                    # compute activation
    cost <- (-1/m)*sum(Y*log(A)+(1-Y)*log(1-A))                                 # compute cost

    # BACKWARD PROPAGATION (TO FIND GRAD)
    dw <-  (1/m)*X %*% t((A-Y))
    db <-  (1/m)*sum(A-Y)

    stopifnot(dim(dw) == dim(w))
    
    #cost <-  np.squeeze(cost)
    
    grads = list(dw=dw,db=db)
    
    results <- list(grads=grads,cost=cost)
    
    return (results)
}
```


```{r}
w <- array(data = c(1,2),dim = c(2,1))
b <- 2
X <- array(data =c(1,3,2,4,-1,-3.2),dim = c(2,3))
Y <- array(data =c(1,0,1),dim = c(1,3))

propagate(w,b,X,Y)
```

### 4.4 - Optimization
You have initialized your parameters.You are also able to compute a cost function and its gradient.
Now, you want to update the parameters using gradient descent.

```{r}
optimize <- function(w, b, X, Y, num.iterations, learning.rate, print.cost = F){
    "
    This function optimizes w and b by running a gradient descent algorithm
    
    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of shape (num_px * num_px * 3, number of examples)
    Y -- true label vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
    num.iterations -- number of iterations of the optimization loop
    learning.rate -- learning rate of the gradient descent update rule
    print.cost -- True to print the loss every 100 steps
    
    Returns:
    params -- dictionary containing the weights w and bias b
    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.
    
    Tips:
    You basically need to write down two steps and iterate through them:
        1) Calculate the cost and the gradient for the current parameters. Use propagate().
        2) Update the parameters using gradient descent rule for w and b.
    "
    
    costs = NULL
    
    for (i in 1:num.iterations){
        
        
        # Cost and gradient calculation (≈ 1-4 lines of code)
        temp.val <- propagate(w, b, X, Y)
        grads <-  temp.val$grads
        cost <-  temp.val$cost

        # Retrieve derivatives from grads
        dw <-  grads$dw
        db <-  grads$db
        
        # update rule (≈ 2 lines of code)
        
        w <-  w-learning.rate*dw
        b <-  b-learning.rate*db

        # Record the costs
        if (i %% 100 == 0){
            costs[i] <- cost
        }
        # Print the cost every 100 training iterations
        if (print.cost & i %% 100 == 0){
            print (paste("Cost after iteration ", i,": ",cost,sep = ""))
        }
    }
    
    params <-  list(w= w,b=b)
    grads <-  list(dw= dw,db= db)
    
    out <- list(params=params,grads=grads,cost=cost)
    
    return (out)
}
```

```{r}
optimize(w, b, X, Y, num.iterations= 100, learning.rate = 0.009, print.cost = F)
```

The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. To implement the `predict()` function. There are two steps to computing predictions:

1. Calculate $$\hat{Y} = A = \sigma(w^T X + b)$$

2. We convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`.We can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). 

```{r}
predict <- function(w, b, X){
    "
    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)
    
    Arguments:
    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
    b -- bias, a scalar
    X -- data of size (num_px * num_px * 3, number of examples)
    
    Returns:
    Y.prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X
    "
    
    m <-  dim(X)[2]
    Y.prediction <-  matrix(data=rep(0,m),ncol = m)
    

    # Compute vector "A" predicting the probabilities of a cat being present in the picture

    A <- sigmoid((t(w) %*% X)+b)                                    # compute activation

    for (i in 1:m){
        
        # Convert probabilities A[1,i] to actual predictions p[1,i]
        if (A[1,i]> 0.5){
            Y.prediction[1,i] <- 1
        }
        else{
            Y.prediction[1,i] <- 0
        }
    }

    return (Y.prediction)
}
```

```{r}
w <- array(data = c(0.1124579,0.23106775),dim = c(2,1))
b <- -0.3
X <- array(data =c(1,1.2,-1.1,2,-3.2,0.1),dim = c(2,3))

predict(w,b,X)
```

## 5 - Merge all functions into a model
We will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.

```{r}
model <- function(X.train, Y.train, X.test, Y.test, num.iterations = 2000, learning.rate = 0.5, print.cost = F){
    "
    Builds the logistic regression model by calling the function you've implemented previously
    
    Arguments:
    X.train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)
    Y.train -- training labels represented by a numpy array (vector) of shape (1, m_train)
    X.test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)
    Y.test -- test labels represented by a numpy array (vector) of shape (1, m_test)
    num.iterations -- hyperparameter representing the number of iterations to optimize the parameters
    learning.rate -- hyperparameter representing the learning rate used in the update rule of optimize()
    print.cost -- Set to true to print the cost every 100 iterations
    
    Returns:
    d -- dictionary containing information about the model.
    "
    
    # initialize parameters with zeros (≈ 1 line of code)
    m <- dim(X.train)[1]
    w <- initialise.to.zero(m)$w
    b <- initialise.to.zero(m)$b

    # Gradient descent (≈ 1 line of code)
    temp <- optimize(w, b, X.train, Y.train, num.iterations, learning.rate,print.cost = T)
    parameters <- temp$params
    grads <-  temp$grads
    costs <-  temp$costs
      
    
    # Retrieve parameters w and b from dictionary "parameters"
    w <-  parameters$w
    b <-  parameters$b
    
    # Predict test/train set examples (≈ 2 lines of code)
    Y.prediction.test <-  predict(w,b,X.test)
    Y.prediction.train <-  predict(w,b,X.train)


    # Print train/test Errors
    print(paste("train accuracy: ",(100 - mean(abs(Y.prediction.train - Y.train)) * 100),sep = ""))
    print(paste("test accuracy:  ",(100 - mean(abs(Y.prediction.test - Y.test)) * 100),sep = ""))

    
    out <-  list(cost=costs,
         Y.prediction.test= Y.prediction.test, 
         Y.prediction.train = Y.prediction.train, 
         w = w, b = b,
         learning.rate = learning.rate,
         num.iterations= num.iterations)
    
    return (out)
}
```
```{r}
train.set.y <- original.data$train.y
test.set.y <- original.data$test.y

d <-  model(train.x.flatten, train.set.y, test.x.flatten, test.set.y, num.iterations = 3000, learning.rate = 0.005, print.cost = T)
```



